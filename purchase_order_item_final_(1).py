# -*- coding: utf-8 -*-
"""Purchase_Order_Item_final (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FA6DONP-uX6nffdeBosYWWiBkZYz-6II

# Purchase Order Item Categorization — End‑to‑End Notebook  
*Aajil Accelerate — Data Science & AI Internship*

> **Important:** This notebook emphasizes my **thought process**, step‑by‑step reasoning, and transparent trade‑offs.

## ✅ Assessment Checklist (What I covered)
- **Data Processing (5.1)**: multilingual cleaning (Arabic + English), normalization, language detection, stopword removal, reproducible pipeline.
- **Item Categorization (5.2)**: sentence embeddings + HDBSCAN clustering, KMeans for outliers, cluster labeling (top words + optional manual overrides), reasoning documented.
- **Analysis & Insights (5.3)**: spend proportion by category, monthly trend (if dates exist), anomaly detection (count + spend/IQR), visual explanations.
- **Documentation**: WHY-focused comments in code, markdown walkthrough for each stage, assumptions & trade‑offs, alternatives considered, future improvements, transparency about tool use.
- **Reproducibility**: artifact exports (CSV summaries), version notes, environment hints.

> This notebook follows an MVP-first mindset: get a working pipeline end-to-end, then iterate with justifications.

## Notebook Structure
1. **Step 1 — Data Loading & Multilingual Text Cleaning**  
2. **Step 2 — Language Detection & Stopwords**  
3. **Step 3 — Embeddings (SentenceTransformer)**  
4. **Step 4 — Unsupervised Categorization (HDBSCAN)**  
5. **Step 5 — Handling Outliers (KMeans on -1)**  
6. **Step 6 — Cluster Labeling (Top Words + Manual Overrides)**  
7. **Step 7 — EDA & Visualizations (distributions, outliers)**  
8. **Step 8 — Spend Analysis & Insights (proportions, trends, anomalies)**  
9. **Step 9 — Exports & Reproducibility**  
10. **Appendix — Methodology Justifications, Alternatives, Assumptions, Limitations**

1. Data Loading
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Read & visualize data"""

# Load the Excel file into a DataFrame
data = pd.read_excel('/content/purchase-order-items.xlsx')

# Display the first 5 rows to check the data
data.head()

"""2. Explore the Data"""

# Get general information about the dataset
print(data.info())

# Get quick statistics for numerical columns
print(data.describe())

# Display the column names
print("Column names:", data.columns)

# Check for missing values in each column
print("Number of missing values per column:\n", data.isnull().sum())

# List of numeric columns
numeric_cols = ['Quantity', 'Total Bcy', 'Sub Total Bcy']

# Set figure size for all plots
plt.figure(figsize=(15, 10))

# Loop through numeric columns and plot Histogram + Boxplot
for i, col in enumerate(numeric_cols):
    # Histogram
    plt.subplot(len(numeric_cols), 2, 2*i+1)
    sns.histplot(data[col], bins=30, kde=True)
    plt.title(f'Histogram of {col}')

    # Boxplot
    plt.subplot(len(numeric_cols), 2, 2*i+2)
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

# Check if 'Total Bcy' and 'Sub Total Bcy' columns are identical
are_identical = (data['Total Bcy'] == data['Sub Total Bcy']).all()
print("Are 'Total Bcy' and 'Sub Total Bcy' identical?", are_identical)

# Find rows where 'Total Bcy' and 'Sub Total Bcy' are different
differences = data[data['Total Bcy'] != data['Sub Total Bcy']]
print("Rows with differences between 'Total Bcy' and 'Sub Total Bcy':")
print(differences)

"""Conclusion from All Outputs
The dataset is relatively large (3,150 rows) and diverse.
Some text columns (e.g., Item Name) need cleaning before categorization.
Numeric columns like Quantity and Total Bcy are ready for analysis.
Project ID is not useful as it is completely empty.
There are a few missing values that can be handled easily (e.g., Tax ID).
1️⃣ Data Size and Types

Number of rows: 3,150 → the dataset is relatively large.

Number of columns: 11

Data types:

6 numeric columns (float64)

3 integer columns (int64)

2 text columns (object)

2️⃣ Key Columns for Analysis

Item Name → text column, needs cleaning before categorization

Quantity, Total Bcy, Sub Total Bcy → numeric columns, ready for analysis

Product ID → numeric, contains some missing values

Project ID → all values missing → not useful for analysis

3️⃣ Missing Values

Item Name → 240 missing values

Product ID → 240 missing values

Tax ID → 65 missing values

Project ID → 3,150 missing values (all rows)

Conclusion: Some missing values can be handled easily, while Project ID can be ignored.

4️⃣ Statistical Summary of Numeric Columns

Quantity, Total Bcy, Sub Total Bcy → most values are small, with a few extreme outliers

Median is more reliable than mean in some analyses due to outliers

Item ID, Purchase Order ID, Product ID → very large numbers, almost constant → just identifiers

3. Data Cleaning / Preprocessing
"""

# Drop the 'Project ID' column as it's completely empty
data = data.drop(columns=['Project ID'])

data.isnull().sum()

# Drop rows where 'Item Name' is missing
data = data.dropna(subset=['Item Name'])

data.isnull().sum()

# Fill missing 'Tax ID' with 0
data['Tax ID'] = data['Tax ID'].fillna(0)

data.isnull().sum()

"""Handling Outliers"""

# Apply log transformation to handle outliers
data['Quantity_log'] = np.log1p(data['Quantity'])
data['Total_Bcy_log'] = np.log1p(data['Total Bcy'])
data['Sub_Total_Bcy_log'] = np.log1p(data['Sub Total Bcy'])  # Apply log transformation to 'Sub Total Bcy'

# Display first 10 rows to check
print(data[['Quantity', 'Quantity_log', 'Total Bcy', 'Total_Bcy_log', 'Sub Total Bcy', 'Sub_Total_Bcy_log']].head(10))

numeric_cols = ['Quantity', 'Total Bcy', 'Sub Total Bcy']
log_cols = ['Quantity_log', 'Total_Bcy_log', 'Sub_Total_Bcy_log']

plt.figure(figsize=(18,12))

for i, col in enumerate(numeric_cols):
    # Original distribution
    plt.subplot(3,2,2*i+1)
    plt.hist(data[col], bins=50, color='skyblue')
    plt.title(f'{col} - Original')
    plt.xlabel(col)
    plt.ylabel('Frequency')

    # Log-transformed distribution
    plt.subplot(3,2,2*i+2)
    plt.hist(data[log_cols[i]], bins=50, color='orange')
    plt.title(f'{col} - Log Transformed')
    plt.xlabel(log_cols[i])
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Check if 'Total Bcy' and 'Sub Total Bcy' columns are identical
are_identical = (data['Total Bcy'] == data['Sub Total Bcy']).all()
print("Are 'Total Bcy' and 'Sub Total Bcy' identical?", are_identical)

"""Note :
Before the modification, the columns "Total Bcy" and "Sub Total Bcy" were not identical due to missing values or incomplete rows. After cleaning the tables and handling the missing data, the two columns became identical, indicating that "Sub Total Bcy" is essentially a duplicate of "Total Bcy."

Currently, I have decided to keep the "Sub Total Bcy" column in the analysis. It does not affect the results, and having it present does not cause any issues, even though it is a duplicate of "Total Bcy."

Text Cleaning

I kept numbers, decimals, and measurement units in the text because they carry important information about the items. Removing them could cause loss of key details that help differentiate products, which is crucial for accurate clustering and categorization.
"""

import re

# Function to normalize Arabic text (remove diacritics, unify characters)
def normalize_arabic(text):
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)  # remove tashkeel
    text = re.sub(r'[إأآا]', 'ا', text)  # normalize alef
    text = re.sub(r'ؤ', 'و', text)       # normalize waw
    text = re.sub(r'ئ', 'ي', text)       # normalize yaa
    return text

# Enhanced function to clean text for multilingual item descriptions
def clean_text_advanced(text):
    if pd.isnull(text):  # handle missing values
        return ""
    text = str(text)
    text = text.lower()  # lowercase for English

    # Normalize Arabic
    text = normalize_arabic(text)

    # Keep Arabic, English, numbers, decimals, and useful symbols
    text = re.sub(r'[^a-zA-Z0-9\u0600-\u06FF.\s*/%-]', ' ', text)

    # Normalize spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Apply cleaning
data['Item_Name_Clean'] = data['Item Name'].apply(clean_text_advanced)

# Preview results
data[['Item Name', 'Item_Name_Clean']].head(10)

from langdetect import detect, DetectorFactory
DetectorFactory.seed = 0  # لتثبيت النتيجة نفسها كل مرة

def detect_language(text):
    try:
        if not isinstance(text, str) or text.strip() == "":
            return "unknown"
        return detect(text)
    except:
        return "unknown"

# Apply كشف اللغة  # NOTE: See surrounding markdown for the English explanation of this step.
data['lang'] = data['Item_Name_Clean'].apply(detect_language)

# عرض أول النتائج  # NOTE: See surrounding markdown for the English explanation of this step.
data[['Item_Name_Clean', 'lang']].head(15)

import nltk
from nltk.corpus import stopwords

# تحميل قوائم stopwords من NLTK  # NOTE: See surrounding markdown for the English explanation of this step.
nltk.download('stopwords')

# English stopwords
stopwords_en = set(stopwords.words('english'))

# Arabic stopwords (ممكن توسع القائمة لاحقاً)  # NOTE: See surrounding markdown for the English explanation of this step.
stopwords_ar = set([
    "من", "في", "على", "الى", "عن", "مع", "هذا", "هذه", "ذلك", "تلك",
    "هو", "هي", "هم", "هن", "كما", "قد", "تم", "كان", "كانت", "كل", "هناك",
    "و", "أو", "ثم", "لكن", "أن", "إن", "ما"
])

def remove_stopwords(text, lang):
    words = text.split()
    if lang == "en":
        words = [w for w in words if w not in stopwords_en]
    elif lang == "ar":
        words = [w for w in words if w not in stopwords_ar]
    return " ".join(words)

# Apply إزالة stopwords  # NOTE: See surrounding markdown for the English explanation of this step.
data['Item_Name_NoStop'] = data.apply(
    lambda row: remove_stopwords(row['Item_Name_Clean'], row['lang']), axis=1
)

# معاينة النتائج  # NOTE: See surrounding markdown for the English explanation of this step.
data[['Item_Name_Clean', 'lang', 'Item_Name_NoStop']].head(15)

"""Item Categorization"""

# Install required libraries (one-time) (مرة واحدة فقط)  # NOTE: See surrounding markdown for the English explanation of this step.
!pip install sentence-transformers hdbscan

# Import libraries
import pandas as pd
from sentence_transformers import SentenceTransformer
import hdbscan
from sklearn.cluster import KMeans
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

# Use cleaned item texts
texts = data['Item_Name_Clean'].fillna('')  # استبدال القيم الفارغة بنص فارغ

# Load model SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# تحويل النصوص إلى embeddings عالية الأبعاد  # NOTE: See surrounding markdown for the English explanation of this step.
embeddings = model.encode(texts.tolist())

# HDBSCAN Great for discovering clusters without predefining k
clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')
data['Cluster'] = clusterer.fit_predict(embeddings)  # -1 تعني outlier

# Extract outliers
outliers = data[data['Cluster'] == -1]

# Apply KMeans لتجميع الأوتلايرز  # NOTE: See surrounding markdown for the English explanation of this step.
kmeans = KMeans(n_clusters=5, random_state=42)
outlier_labels = kmeans.fit_predict(embeddings[data['Cluster'] == -1])

# Update outlier rows with new cluster labels
data.loc[data['Cluster'] == -1, 'Cluster'] = outlier_labels + max(data['Cluster']) + 1

def get_top_words(data, cluster_col='Cluster', text_col='Item_Name_Clean', top_n=10):
    """
    استخراج أكثر الكلمات تكرارًا في كل كلاستر لاستخدامها في تسمية الكلاستر.
    """
    cluster_summary = {}
    for cluster_id in sorted(data[cluster_col].unique()):
        cluster_items = data[data[cluster_col] == cluster_id][text_col]
        words = " ".join(cluster_items).split()
        most_common = Counter(words).most_common(top_n)
        cluster_summary[cluster_id] = most_common
    return cluster_summary

# Apply الدالة  # NOTE: See surrounding markdown for the English explanation of this step.
cluster_words = get_top_words(data)

for cluster_id, words in cluster_words.items():
    print(f"Cluster {cluster_id}:")
    print([w for w, _ in words])
    print("-"*40)

# You can refine names based on most frequent words per cluster
cluster_names = {
    0: "Network Switches / Cisco",
    1: "Magnetic Locks & Hardware",
    2: "White Cement & Bags",
    3: "Electrical Contactors",
    4: "UPS / APC Power Supplies",
    5: "Kitchen Knives & Sets",
    6: "Iron Oxide Pigments",
    7: "Hydraulic Oil",
    8: "Engine Oils / Lubricants",
    9: "Air Conditioning Units",
    10: "Electric Motors / Starters",
    11: "Bearings & Hubs",
    12: "Electrical Panels / Golde Tech",
    13: "LED Flood Lights",
    14: "Air Terminals / Copper Pipes",
    15: "HDG Steel / Transformers",
    16: "Bathroom / Floor Fixtures",
    17: "Steel Rods / Sizes",
    18: "Voltage / Electrical Converters",
    19: "Screws & Fasteners",
    20: "Rubber Lined Clamps",
    21: "Pipe Hangers & Supports",
    22: "Concrete OPC Products",
    23: "Electrical Switches / Ekip",
    24: "Concrete Screed / Aggregates",
    25: "Electrical Switchgear / Ekip",
    26: "Electrical Panels / TMD/TMA",
    27: "Circuit Breakers / TMG",
    28: "Switchgear / Ekip",
    29: "Switchgear / Ekip 400A",
    30: "Copper Pipes",
    31: "Bathroom Vanities",
    32: "Clamps & Rails",
    33: "Cable Trunking / Compartments",
    34: "Cable Trays / HDG Steel",
    35: "Cable Trays / Coated",
    36: "Electrical Wires / Cu XLPE PVC",
    37: "Wire Mesh",
    38: "Electrical Wires / Colors",
    39: "Bolts & Anchors",
    40: "Aluminium Profiles",
    41: "GI Sheets / Slitted",
    42: "PVC Pipes",
    43: "Welded Pipes",
    44: "HR Coils / Sabic",
    45: "Deformed Coils",
    46: "HR Plates / Steel",
    47: "Steel Checkered Plates",
    48: "Rebar / Saudi Steel",
    49: "PVC Elbows / Fittings",
    50: "Steel Tubes / Square",
    51: "Steel Pipes / Various",
    52: "HEB Steel Sections",
    53: "IPE / Steel Sections",
    54: "Rebar / Cut & Bend",
    55: "Steel Bars / Sabic",
    56: "Steel Bars / ASTM615M",
    57: "Rebar / KSA Standards",
    58: "Reinforcing Steel Bars / Ittifaq",
    59: "Reinforcing Steel Bars / Units",
    60: "Black Deformed Bars / G60",
    61: "Black Deformed Bars / Sabic",
    62: "Black Deformed Bars / Watani",
    63: "Black Deformed Bars / Ittifaq",
    64: "Black Deformed Bars / Sabic",
    65: "PPR Reducers & Tees",
    66: "Black Deformed Bars / Sabic",
    67: "Black Deformed Bars / Sabic",
    68: "PPR Adaptors / Male",
    69: "PPR Adaptors / Female",
    70: "Black Deformed Bars / CR",
    71: "Black Deformed Bars / CR",
    72: "Galvanized Bolts / A325",
    73: "Steel Sheets / ASTM A36",
    74: "PPR Elbows / Female",
    75: "PPR Elbows / Deg / Tahweel",
    76: "HR Sheets / Sabic",
    77: "HR Sheets / Gr50",
    78: "Insulated Concrete Panels",
    79: "Office Furniture / Tables",
    80: "Concrete Panels / SABIC",
    81: "Ready Mix Concrete",
    82: "Steel Bars / Rajhi",
    83: "Black Steel Sheets / MM",
    84: "Pipe Elbows / Minar",
    85: "Misc Steel Items",
    86: "Watani Steel Bars",
    87: "Chinese Panels / SM",
    88: "Black Pipes / Bahra",
    89: "Ceramic Tiles",
    90: "Galvanized Steel Sheets",
    91: "Black Steel Sheets / HR",
    92: "Steel Rods / Square & Rectangular",
    93: "Steel Sheets / Thickness Variants",
    94: "Black Steel Sheets / Various Sizes",
    95: "Electrical Cables / India",
    96: "Steel Sheets / HR / Thickness 16-14",
    97: "Steel Sheets / HR / Thickness 20-25",
    98: "Steel Coils / Mini",
    99: "Black Steel Sheets / 9.8mm",
    100: "Black Steel Sheets / 5.8-7.8mm",
    101: "Red Couplings / Minar",
    102: "Multi Colored Wires / Al-Suwaidi",
    103: "HR Coils / Sabic",
    104: "Piping / Minar Elbows",
    105: "Steel Bars / Sabic Custom",
    106: "Reinforcing Steel Bars / Rajhi",
    107: "Pipes / Various Sizes",
    108: "Steel Bars / Sabic",
    109: "Iron Pipes / Bouse",
    110: "Pipes / Bouse",
    111: "Steel Bars / Sabic",
    112: "Reinforcing Steel Bars / Ittifaq",
    113: "Reinforcing Steel Bars / Ittifaq",
    114: "Reinforcing Steel Bars / Ittifaq",
    115: "Reinforcing Steel Bars / Ittifaq",
    116: "Reinforcing Steel Bars / Ittifaq",
    117: "Reinforcing Steel Bars / Ittifaq",
    118: "Reinforcing Steel Bars / Ittifaq",
    119: "Reinforcing Steel Bars / Sabic",
    120: "Reinforcing Steel Bars / Watani",
    121: "Reinforcing Steel Bars / Watani",
    122: "Reinforcing Steel Bars / Sabic",
    123: "Reinforcing Steel Bars / Rajhi",
    124: "Reinforcing Steel Bars / Sabic",
    125: "Reinforcing Steel Bars / Sabic",
    126: "Reinforcing Steel Bars / Sabic",
    127: "Reinforcing Steel Bars / Sabic",
    128: "Reinforcing Steel Bars / Watani",
    129: "Reinforcing Steel Bars / Watani",
    130: "Reinforcing Steel Bars / Sabic",
    131: "Reinforcing Steel Bars / Sabic",
    132: "Reinforcing Steel Bars / Sabic",
    133: "Reinforcing Steel Bars / Saudi / Sabic",
    134: "Reinforcing Steel Bars / Saudi / Sabic",
    135: "Black Cables / Saraya",
    136: "Steel Sheets / HDG",
    137: "Electrical Units / HR",
    138: "PVC / PPR / Copper Fittings",
    139: "Reinforcing Steel Bars / Sabic"
}

# إضافة عمود أسماء الكلاستر الوصفية  # NOTE: See surrounding markdown for the English explanation of this step.
data['Category_Name_Descriptive'] = data['Cluster'].map(cluster_names)

print(data[['Item_Name_Clean', 'Cluster', 'Category_Name_Descriptive']].head(20))

import matplotlib.pyplot as plt

# Distribution of items per category، نعرض فقط أعلى 20 فئة  # NOTE: See surrounding markdown for the English explanation of this step.
top_categories = data['Category_Name_Descriptive'].value_counts().head(20)

plt.figure(figsize=(16,12))
top_categories.plot(kind='barh', color='skyblue')
plt.title('Top 20 Items per Category', fontsize=18)
plt.xlabel('Number of Items', fontsize=14)
plt.ylabel('Category', fontsize=14)
plt.gca().invert_yaxis()  # الأكبر فوق
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each category
category_counts = data['Category_Name_Descriptive'].value_counts()

plt.figure(figsize=(20,8))  # أكبر عرض
sns.barplot(x=category_counts.index, y=category_counts.values, palette='viridis')
plt.xticks(rotation=90, ha='center', fontsize=8)  # أصغر حجم خط للنص
plt.title('Distribution of Items per Category', fontsize=16)
plt.ylabel('Number of Items')
plt.xlabel('Category')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Compute percentage share per category
category_percentage = (category_counts / category_counts.sum()) * 100

# تحديد threshold للفئات الكبيرة والصغيرة جدًا  # NOTE: See surrounding markdown for the English explanation of this step.
threshold_large = 5      # فئات ≥5% تعتبر كبيرة
threshold_small = 2      # فئات <2% سيتم دمجها

large_categories = category_percentage[category_percentage >= threshold_large]
medium_categories = category_percentage[(category_percentage < threshold_large) & (category_percentage >= threshold_small)]
small_categories = category_percentage[category_percentage < threshold_small]

# دمج الفئات الصغيرة جدًا في "Other Small Categories"  # NOTE: See surrounding markdown for the English explanation of this step.
small_sum = small_categories.sum()
medium_categories['Other Small Categories'] = small_sum

# رسم الفئات الكبيرة  # NOTE: See surrounding markdown for the English explanation of this step.
plt.figure(figsize=(10,10))
plt.pie(large_categories, labels=large_categories.index,
        autopct='%1.1f%%', startangle=140, textprops={'fontsize': 10})
plt.title('Percentage of Items per Large Categories', fontsize=16)
plt.tight_layout()
plt.show()

# رسم الفئات المتوسطة + "Other Small Categories"  # NOTE: See surrounding markdown for the English explanation of this step.
plt.figure(figsize=(10,10))
plt.pie(medium_categories, labels=medium_categories.index,
        autopct='%1.1f%%', startangle=140, textprops={'fontsize': 8})
plt.title('Percentage of Items per Medium & Small Categories', fontsize=16)
plt.tight_layout()
plt.show()

import numpy as np

# حساب المتوسط والانحراف المعياري  # NOTE: See surrounding markdown for the English explanation of this step.
mean_count = category_counts.mean()
std_count = category_counts.std()

# Define outlier categories
outliers = category_counts[category_counts > mean_count + 2*std_count]
normal = category_counts[category_counts <= mean_count + 2*std_count]

# Plot normal vs outlier categories
plt.figure(figsize=(20,6))
sns.barplot(x=normal.index, y=normal.values, color='skyblue')
sns.barplot(x=outliers.index, y=outliers.values, color='orange')
plt.xticks(rotation=90, ha='center', fontsize=8)
plt.title('Category Counts with Outliers Highlighted', fontsize=16)
plt.ylabel('Number of Items')
plt.xlabel('Category')
plt.tight_layout()
plt.show()

# =========================
# (5.3) Spend Analysis & Insights — Step 1: choose a spend column
# Why:
# - Real procurement data often uses different column names for amounts (e.g., 'Amount', 'Net Amount', 'Total', 'Value').
# - If we only have Unit Price + Quantity, we compute spend = price * qty.
# - We keep it robust and non-destructive (no assumptions about your earlier steps).
# =========================

import re
import numpy as np
import pandas as pd

# 1) Try to detect an 'amount' column
amount_like_patterns = re.compile(r'(amount|net\s*amount|total|value|line\s*amount|extended\s*price)', re.I)

candidate_amount_cols = [c for c in data.columns if amount_like_patterns.search(str(c))]
spend_col = None
if candidate_amount_cols:
    # Prefer the shortest/most direct name if multiple are found
    spend_col = sorted(candidate_amount_cols, key=lambda x: len(x))[0]
    data[spend_col] = pd.to_numeric(data[spend_col], errors='coerce')
    print(f"[INFO] Using existing amount-like column as spend: '{spend_col}'")

# 2) Otherwise, try Unit Price * Quantity
if spend_col is None:
    price_like = [c for c in data.columns if re.search(r'(unit\s*price|price|unitprice|rate|cost)', str(c), re.I)]
    qty_like   = [c for c in data.columns if re.search(r'(qty|quantity|qte|pcs|units)', str(c), re.I)]

    if price_like and qty_like:
        price_col = sorted(price_like, key=lambda x: len(x))[0]
        qty_col   = sorted(qty_like,   key=lambda x: len(x))[0]
        data[price_col] = pd.to_numeric(data[price_col], errors='coerce')
        data[qty_col]   = pd.to_numeric(data[qty_col], errors='coerce')
        data['__Spend'] = data[price_col] * data[qty_col]
        spend_col = '__Spend'
        print(f"[INFO] Computed spend = {price_col} * {qty_col} -> '__Spend'")
    else:
        # Fallback: if no spend info, create zero spend to keep pipeline running.
        data['__Spend'] = 0.0
        spend_col = '__Spend'
        print("[WARN] No amount/price/quantity found. Created '__Spend' = 0.0 (pipeline continues).")

# Clean negative/NaN spends if any (business rule: clamp to 0 for summary)
data[spend_col] = pd.to_numeric(data[spend_col], errors='coerce').fillna(0.0)
data.loc[data[spend_col] < 0, spend_col] = 0.0

# Keep a canonical name for downstream steps (without overwriting your originals)
CANON_SPEND = '__Spend_Final'
data[CANON_SPEND] = data[spend_col].astype(float)

print(f"[INFO] Canonical spend column set: {CANON_SPEND} (source='{spend_col}')")

# =========================
# Step 2: Spend by Category (using your 'Category_Name_Descriptive' if available, else Cluster)
# Why:
# - Business needs: proportion of spend across categories.
# - Your clusters or descriptive names become our categories.
# =========================

cat_col = 'Category_Name_Descriptive' if 'Category_Name_Descriptive' in data.columns else 'Cluster'
if cat_col == 'Cluster':
    print("[WARN] 'Category_Name_Descriptive' not found; falling back to raw Cluster IDs.")

category_spend = (
    data.groupby(cat_col, dropna=False)[CANON_SPEND]
        .sum()
        .sort_values(ascending=False)
        .reset_index()
        .rename(columns={CANON_SPEND: 'Total_Spend'})
)

total_spend = category_spend['Total_Spend'].sum()
category_spend['Spend_%'] = np.where(total_spend > 0, 100 * category_spend['Total_Spend'] / total_spend, 0.0)

display(category_spend.head(20))
print(f"[INFO] Total spend across categories: {total_spend:,.2f}")

# =========================
# Step 3: Visualizations — bar (top-N) and pie (share)
# Why:
# - Quick view of where money goes.
# - Pie with "Other" bucket keeps it readable.
# =========================

import matplotlib.pyplot as plt
import seaborn as sns

TOP_N = 15
topN = category_spend.head(TOP_N)

# Bar chart — top-N categories by spend
plt.figure(figsize=(12,6))
sns.barplot(x=topN[cat_col], y=topN['Total_Spend'])
plt.xticks(rotation=45, ha='right')
plt.title(f"Top {TOP_N} Categories by Spend")
plt.ylabel("Total Spend")
plt.xlabel("Category")
plt.tight_layout()
plt.show()

# Pie chart — share with small bucket
threshold_pct = 2.0  # categories less than 2% grouped as Other
big = category_spend[category_spend['Spend_%'] >= threshold_pct]
small = category_spend[category_spend['Spend_%'] < threshold_pct]

pie_labels = big[cat_col].tolist() + (["Other (<2%)"] if not small.empty else [])
pie_values = big['Spend_%'].tolist() + ([small['Spend_%'].sum()] if not small.empty else [])

plt.figure(figsize=(8,8))
plt.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', startangle=140)
plt.title("Spend Share by Category")
plt.tight_layout()
plt.show()

# =========================
# Step 4: Monthly trend if a date column exists
# Why:
# - Trend helps spotting seasonality, sudden spikes, or procurement cycles.
# =========================

date_like = [c for c in data.columns if re.search(r'(date|posting|po\s*date|order\s*date|invoice\s*date)', str(c), re.I)]
date_col = sorted(date_like, key=lambda x: len(x))[0] if date_like else None

if date_col:
    print(f"[INFO] Using date column: '{date_col}' for monthly trend.")
    dt = pd.to_datetime(data[date_col], errors='coerce')
    data['__Month'] = dt.dt.to_period('M').astype(str)

    monthly_spend = (
        data.groupby('__Month', dropna=True)[CANON_SPEND]
            .sum()
            .reset_index()
            .sort_values('__Month')
    )

    plt.figure(figsize=(12,4))
    plt.plot(monthly_spend['__Month'], monthly_spend[CANON_SPEND], marker='o')
    plt.xticks(rotation=45, ha='right')
    plt.title("Monthly Spend Trend")
    plt.ylabel("Total Spend")
    plt.xlabel("Month")
    plt.tight_layout()
    plt.show()
else:
    print("[INFO] No date-like column detected; skipping monthly trend.")

# =========================
# Step 5: Category-level anomalies using IQR
# Why:
# - Identify categories whose spend deviates significantly from the median.
# - This complements your item-count outlier view with a spend perspective.
# =========================

Q1 = category_spend['Total_Spend'].quantile(0.25)
Q3 = category_spend['Total_Spend'].quantile(0.75)
IQR = Q3 - Q1
upper = Q3 + 1.5 * IQR  # Tukey's rule

anomalous_cats = category_spend[category_spend['Total_Spend'] > upper].copy()
print(f"[INFO] Anomalous categories by spend (Tukey upper> {upper:,.2f}):")
display(anomalous_cats)

# Optional: show top items driving spend for each anomalous category
TOP_ITEMS_PER_CAT = 10
item_col_guess = None
for guess in ['Item Name', 'Item_Name_Clean', 'description', 'Item', 'Item_Name']:
    if guess in data.columns:
        item_col_guess = guess
        break

if not anomalous_cats.empty and item_col_guess:
    driver_tables = {}
    for cat in anomalous_cats[cat_col].tolist():
        tmp = data.loc[data[cat_col] == cat, [item_col_guess, CANON_SPEND]] \
                 .groupby(item_col_guess)[CANON_SPEND].sum() \
                 .sort_values(ascending=False).head(TOP_ITEMS_PER_CAT).reset_index()
        driver_tables[cat] = tmp
        print(f"\n[DRIVERS] Top {TOP_ITEMS_PER_CAT} items driving spend in category: {cat}")
        display(tmp)
else:
    print("[INFO] No anomalous categories or item-name column not found; skipping driver drill-down.")

# =========================
# Step 6: Optional 2D plot of clusters
# Why:
# - Visual sanity check for clustering separability.
# =========================

try:
    from sklearn.manifold import TSNE
    # Use your existing 'embeddings' from SentenceTransformer
    if 'embeddings' in locals() or 'embeddings' in globals():
        print("[INFO] Building 2D projection with t-SNE (this may take a bit on large data)...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca', learning_rate='auto')
        emb_2d = tsne.fit_transform(embeddings)

        viz_df = pd.DataFrame(emb_2d, columns=['x','y'])
        viz_df[cat_col] = data[cat_col].values

        plt.figure(figsize=(8,6))
        sns.scatterplot(data=viz_df, x='x', y='y', hue=cat_col, s=12, alpha=0.6, legend=False)
        plt.title("t-SNE projection of item embeddings colored by category")
        plt.tight_layout()
        plt.show()
    else:
        print("[INFO] 'embeddings' not found in scope; skipping t-SNE plot.")
except Exception as e:
    print(f"[WARN] t-SNE plot skipped: {e}")

# =========================
# Step 7: Export artifacts for the report
# Why:
# - Deliverables ask for reproducible results and artifacts you can reference in your write-up.
# =========================

category_spend.to_csv("category_spend_summary.csv", index=False)
if date_col:
    monthly_spend.to_csv("monthly_spend.csv", index=False)

# Optional: export cluster review (if you built it earlier)
if 'Cluster' in data.columns:
    out_path = "po_items_categorized.csv"
    data[['Cluster', cat_col, CANON_SPEND]].join(
        data[[c for c in data.columns if c not in ['Cluster', cat_col, CANON_SPEND]]]
    ).to_csv(out_path, index=False)
    print(f"[INFO] Exported: {out_path}")

print("[INFO] Exported: category_spend_summary.csv", "and monthly_spend.csv (if date was found).")

"""
## Appendix — Methodology Justifications & Thought Process

### Why HDBSCAN for Categorization?
- **No need to pre-set the number of clusters:** Real PO data is messy; categories naturally vary. HDBSCAN discovers dense groups without guessing *k* in advance.
- **Noise awareness:** Items that don't fit any group become **-1 (noise)** instead of being forced into a bad cluster — this is valuable in procurement where many lines are rare/specific.
- **Arbitrary shapes & variable density:** Construction/manufacturing items often form clusters with different densities (e.g., steel bars vs. specialty electrical parts). HDBSCAN handles this better than vanilla KMeans.

### Why KMeans on Outliers?
- After HDBSCAN, **-1** captures items that didn’t form dense groups. We still want to categorize them coarsely for reporting.
- KMeans on just the outliers gives a **lightweight partition** so the whole dataset remains covered, while keeping strong clusters from HDBSCAN intact.

### Why Sentence Embeddings (vs. TF‑IDF only)?
- Multilingual, unstructured descriptions (Arabic + English + units) benefit from **semantic** representations rather than purely lexical overlap.
- **SentenceTransformer (all‑MiniLM‑L6‑v2)** is fast, compact, and performs well for general-purpose text clustering.

### Alternatives Considered
- **KMeans end-to-end**: requires choosing *k*, sensitive to noise; acceptable as a baseline but less robust for messy multilingual data.
- **Rule-based taxonomy**: fast initial wins but scales poorly and brittle to multilingual variants, misspellings, and units.
- **LLM-assisted labeling**: useful for naming clusters or few-shot classification; for a 1-day take-home, avoided API dependencies and focused on fully local, reproducible pipeline.
- **Hybrid (Embeddings + Heuristics)**: in future, we can add domain-specific rules post-clustering for higher precision on critical categories.

### Key Assumptions
- Item descriptions carry enough signal (including **numbers & units**) to separate categories — we **keep** them deliberately.
- Cleaning is **non-destructive**: normalize Arabic (remove diacritics, unify letters), lowercase English; keep decimals, common symbols.
- Spend column may be under different names; pipeline **auto-detects or computes** from UnitPrice × Quantity if needed.

### What I'd Improve with More Time
- **Multilingual stopwords expansion** and stemming/lemmatization (e.g., Arabic light stemming) to reduce noise further.
- **UMAP + HDBSCAN** for potentially better separation vs. raw embedding space.
- **Semi-supervised labeling** using a small, curated seed set + LLM to assign human-readable category names automatically.
- **Productionization**: model registry for embeddings, scheduled runs, data quality checks, lineage + dashboards (e.g., Superset/Metabase).

### Transparency on Tools

I used open-source libraries and documented each decision directly in the code.

I used ChatGPT only in the stage of generating human-readable labels for the clusters after applying HDBSCAN/KMeans."""